{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd79475-3a29-4d21-b93c-a58f2e2c0204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "103bb655-c110-4cb0-bcd9-83b60eeb7589",
   "metadata": {},
   "source": [
    "Config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05d43d-7131-4215-9cdb-f44dec15f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RbcParams:\n",
    "    \"\"\"RBC model parameters\"\"\"\n",
    "    \n",
    "    α:     float = 0.36     # Capital share\n",
    "    η:     float = 0.34     # Labor weight\n",
    "    ρ:     float = 0.918    # TFP shock persistence\n",
    "    β:     float = 0.96     # Discount factor\n",
    "    δ:     float = 0.1      # Depreciation rate\n",
    "    μ_ϵ:   float = 0.0      # Mean of TFP\n",
    "    σ_ϵ:   float = 0.014    # Std of TFP\n",
    "    \n",
    "\n",
    "    def __post_init(self):\n",
    "        \"\"\"Full depreciation case δ=1.0, closed-form solution\"\"\"\n",
    "        self.γ = ((1.0 - self.α)*self.η)/((1.0 - self.α*self.β)*(1 - self.η))\n",
    "\n",
    "        # Labor supply when δ=1.0\n",
    "        self.h_bar = self.γ/(1 + self.γ)\n",
    "\n",
    "    def prod_fn(self, a_t, k_t, h_t):\n",
    "        \"\"\"Cobb-Douglas production function\"\"\"\n",
    "        return a_t * (k_t ** self.α) * (h_t ** (1-self.α))\n",
    "\n",
    "    \n",
    "    def consumption_full_dep(self, y_t):\n",
    "        \"\"\"Closed-form solution for consumption under full depreciation\"\"\"\n",
    "        return (1 - self.α * self.β) * y_t\n",
    "    \n",
    "    def Ξ_full_dep(self, neural_net):\n",
    "        \"\"\"Loss function under full depreciation\"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fcb4d-3245-4c5b-9334-006df58578a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NetworkParams:\n",
    "    \"\"\" Neural net parameters\"\"\"\n",
    "\n",
    "    in_dim:  int = 2        # input dimension (k_t, a_t)\n",
    "    hid_dim: List[int] = field(default_factory=lambda: [16]) # Hidden layer\n",
    "    out_dim: int = 1  # output dimension (consumption share)\n",
    "    act_fn: Union[str, List[str]] = \"relu\"  # Activation function(s)\n",
    "    dropout_rate: float = 0.0 # Dropout rate\n",
    "    num_batch: int = 100   # Batch size\n",
    "    num_epoch: int = 5000   # Epoch size\n",
    "    optimizer: str = \"adam\"\n",
    "    out_bound: float = 1e-6  # output bound\n",
    "    lr: float = 1e-3 # learning rate\n",
    "    freq_gamma: float = 0.85    # Exponential Decay rate for Exponential Scheduler\n",
    "    LRschedule: str = \"exponential\"   # Learning rate scheduler\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate activation functions, optimizers and scheduler\"\"\"\n",
    "\n",
    "        # Valid activtion functions\n",
    "        valid_activations = [\"relu\", \"tanh\", \"sigmoid\"]\n",
    "\n",
    "        if isinstance(self.act_fn, str):\n",
    "            if self.act_fn.lower() not in valid_activations:\n",
    "                raise ValueError(f\"Activation function must be one of: {valid_activations}\")\n",
    "        else:\n",
    "            for act in self.act_fn:\n",
    "                if act.lower() not in valid_activations:\n",
    "                    raise ValueError(f\"Activation function must be one of: {valid_activations}\")\n",
    "\n",
    "        # Valid optimizers\n",
    "        valid_optimizers = [\"adam\", \"sgd\"]\n",
    "        if self.optimizer.lower() not in valid_optimizers:\n",
    "            raise ValueError(f\"Optimizer must be one of: {valid_optimizers}\")\n",
    "\n",
    "        valid_schedulers = [\"exponential\"]\n",
    "        if self.LRscheduler.lower() not in valid_schedulers:\n",
    "            raise ValueError(f\"Learning rate scheduler must be one of {valid_schedulers}\")\n",
    "        \n",
    "    def get_act_fn(self, idx=0) -> nn.Module:\n",
    "        \"\"\"Convert activation string to PyTorch activation function\"\"\"\n",
    "        if isinstance(self.act_fn, list):\n",
    "            if idx >= len(self.act_fn):\n",
    "                act_name = self.act_fn[-1]\n",
    "            else:\n",
    "                act_name = self.act_fn[idx]\n",
    "        else:\n",
    "            act_name = self.act_fn\n",
    "\n",
    "        act_name = act_name.lower()\n",
    "        if act_name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        elif act_name == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        elif act_name == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "            \n",
    "    def get_optimizer(self, neural_net) -> torch.optim.Optimizer:\n",
    "        \"\"\"Convert optimizer string to PyTorch Optimizer\"\"\"\n",
    "        opt_name = self.optimizer.lower()\n",
    "        if opt_name == \"adam\":\n",
    "            return torch.optim.Adam(neural_net.parameters(), lr = self.lr)\n",
    "        if opt_name == \"sgd\":\n",
    "            return torch.optim.SGD(neural_net.parameters(), lr= self.lr)\n",
    "\n",
    "    def get_scheduler(self, optimizer) -> torch.optim.lr_scheduler:\n",
    "        \"\"\"Convert LRscheduler string to PyTorch scheduler\"\"\"\n",
    "        scheduler_name = self.LRscheduler.lower()\n",
    "        if scheduler_name == \"exponential\":\n",
    "            return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = self.freq_gamma)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235ac1c-9738-47b9-b0bd-17dc9ab2c89f",
   "metadata": {},
   "source": [
    "Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74227f5b-759a-4cb8-b0ca-9dc60f2cd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"Neural Network model with configurable architecture\"\"\"\n",
    "    def __init__(self, params: NetworkParams):\n",
    "        \"\"\"Initialize neural network based on provided parameters\"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "\n",
    "        # Initialize layers\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(params.in_dim, params.hid_dim[0]))\n",
    "        if params.dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(p=params.dropout_rate))\n",
    "        layers.append(params.get_act_fn(0))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for i in range(1, len(params.hid_dim)):\n",
    "            layers.append(nn.Linear(params.hid_dim[i-1], params.hid_dim[i]))\n",
    "            if params.dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(p=params.dropout_rate))\n",
    "            layers.append(params.get_act_fn(i))\n",
    "\n",
    "        # Output Layers\n",
    "        layers.append(nn.Linear(params.hid_dim[-1], params.out_dim))\n",
    "\n",
    "        # put the layers together\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    " \n",
    "        out = self.layers(x)\n",
    "\n",
    "        # Apply sigmoid to output\n",
    "        ζ_0 = torch.sigmoid(out)\n",
    "\n",
    "        # bound the output\n",
    "        ζ_1 = torch.minimum(\n",
    "            torch.maximum(ζ_0, torch.tensor([self.params.out_bound])),\n",
    "            torch.tensor([1.0 - self.params.out_bound])\n",
    "        )\n",
    "\n",
    "        return ζ_1\n",
    "\n",
    "\n",
    "\n",
    "    def normalized_forward(self, params: RbcParams, x):\n",
    "        \"\"\"Forward pass with normalized input\"\"\"\n",
    "\n",
    "        # Capital input\n",
    "        k_t = x[:, 0].unsqueeze(1)\n",
    "\n",
    "        # TFP input\n",
    "        a_t = x[:, 1].unsqueeze(1)\n",
    "\n",
    "        # Normalized capital input\n",
    "        k_t_norm = \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def initialization(params: NetworkParams, loss_freq = 1000):\n",
    "    \"\"\"Initialize the neural network with full depreciation case as a guess\"\"\"\n",
    "\n",
    "        # Instantiate a neural net\n",
    "        net = NeuralNetwork(params)\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "\n",
    "        # get optimizer\n",
    "        optimizer = params.get_optimizer(net)\n",
    "\n",
    "        # get scheduler\n",
    "        scheduler = params.get_scheduler(optimizer)\n",
    "\n",
    "        # Initialize epoch size loss tensor\n",
    "        loss_epoch = torch.zeros(params.num_epoch)\n",
    "    \n",
    "        for i in range(0, params.num_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            loss \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37567a26-4dba-450a-853b-4d48cb419e95",
   "metadata": {},
   "source": [
    "Build loss function for the RBC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5ff9b-07d7-4019-b909-1692fc76a198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2cb1e-aad9-4790-9499-c9a7f129ae4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
