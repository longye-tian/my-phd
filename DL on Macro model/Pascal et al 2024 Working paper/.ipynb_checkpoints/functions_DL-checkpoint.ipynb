{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2eef2d",
   "metadata": {},
   "source": [
    "# Functions DL\n",
    "\n",
    "This notebook contains the functions used to solve the DSGE model using a Deep Learning (DL) approach. See the main notebook `DL_DSGE_QuantEcon.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d73cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "\n",
    "json.loads(getSystemInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee897d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    \"\"\"Convert a scipy sparse matrix to a tf sparse tensor.\"\"\"\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# create a nn class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "    \n",
    "# quantile function for lognormal\n",
    "def quantile_norm(mu, sigma, p):\n",
    "    return mu + np.sqrt(2*(sigma**2))*erfinv(2*p - 1)\n",
    "\n",
    "# quantile function for lognormal\n",
    "def quantile_lognorm(mu, sigma, p):\n",
    "    return np.exp(mu + np.sqrt(2*(sigma**2))*erfinv(2*p - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ffec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_vec(nb_draws, params):\n",
    "    \n",
    "    if params.x_distribution == \"Uniform\":\n",
    "        if params.use_Sobol == False:\n",
    "            k_min_1 = ((params.x_low - params.x_high) * torch.rand(nb_draws) + params.x_high).unsqueeze(1)\n",
    "        else:\n",
    "            #Very slow if T is large\n",
    "            k_min_1 = ((params.x_low - params.x_high) * params.soboleng.draw(nb_draws) + params.x_high)\n",
    "    else:\n",
    "        k_min_1 = torch.normal(mean=params.mean_K_linear, std=params.stdev_K_linear, size=(nb_draws,)).unsqueeze(1)\n",
    "    # B. Values for a_t. Draw in ergodic set.\n",
    "    a_t = params.distrib_a_torch.sample((nb_draws,))\n",
    "    # Concat A and B\n",
    "    k_min_1_and_a_t = torch.column_stack([k_min_1, a_t])\n",
    "    \n",
    "    return k_min_1_and_a_t\n",
    "\n",
    "def generate_innovation_vec(nb_draws, params):\n",
    "    innovation_vector = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "    return innovation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ξ_torch(neural_net, params): # objective function for training\n",
    "    # neural net: a pytorch neural network\n",
    "    # params: a params object\n",
    "    \n",
    "    # I.randomly drawing current states    \n",
    "    k_min_1_and_a_t = generate_state_vec(params.T, params)\n",
    "    \n",
    "    # II. randomly drawing innovations\n",
    "    e1 = generate_innovation_vec(params.T, params)\n",
    "    e2 = generate_innovation_vec(params.T, params)\n",
    "    \n",
    "    # III. residuals for n random grid points under 2 realizations of shocks\n",
    "    R1 = Residuals_torch(neural_net, params, k_min_1_and_a_t, e1)\n",
    "    R2 = Residuals_torch(neural_net, params, k_min_1_and_a_t, e2)\n",
    "\n",
    "    # construct all-in-one expectation operator\n",
    "    R_squared = R1*R2 \n",
    "    \n",
    "    # Mean square error:\n",
    "    return torch.mean(R_squared)\n",
    "\n",
    "# Residual function\n",
    "# v2 seems to result in slightly better performance\n",
    "# v1 has the advantage of giving you directly the model's error (take the square root)\n",
    "def Residuals_torch(neural_net, params, state_vec, e_r, tol=torch.tensor([1e-6]), penalty_factor = torch.tensor([100.0]), debug_n = False, version_resid = 1):\n",
    "    # Current period\n",
    "    k_min_1 = state_vec[:,0].unsqueeze(1)\n",
    "    a_t =  state_vec[:,1].unsqueeze(1)\n",
    "    # fix n to cst val \n",
    "    if debug_n == False:\n",
    "        #consumption and hours worked today\n",
    "        c_t, n_t = model_normalized(state_vec, neural_net, params)\n",
    "    else:\n",
    "        c_t, n_t = model_normalized(state_vec, neural_net, params)\n",
    "        # overwritte value for n\n",
    "        n_t = params.n_cst * torch.ones(c_t.shape)\n",
    "    y_t = params.f(a_t, k_min_1, n_t) #production\n",
    "    # BC implies investment decision\n",
    "    x_t = y_t - c_t\n",
    "    # capital stock next period\n",
    "    # investment scaled by shock\n",
    "    k_t = (1 - params.delta)*k_min_1 + x_t\n",
    "    penalty_t = penalty_factor*torch.maximum(torch.tensor([0.0]), -k_t)**2\n",
    "    k_t = torch.maximum(tol, k_t)\n",
    "    # Next period\n",
    "    # transition for a_t\n",
    "    a_tomorrow = torch.exp(params.rho*torch.log(a_t) + e_r)\n",
    "    input_tomorrow = torch.column_stack([k_t, a_tomorrow])\n",
    "    if debug_n == False:\n",
    "        # consumption and hours worked tomorrow\n",
    "        c_tomorrow, n_tomorrow = model_normalized(input_tomorrow, neural_net, params)\n",
    "    # fix n to cst val \n",
    "    else:\n",
    "        c_tomorrow, n_tomorrow = model_normalized(input_tomorrow, neural_net, params)\n",
    "        n_tomorrow = params.n_cst * torch.ones(c_t.shape)\n",
    "    y_tomorrow = params.f(a_tomorrow, k_t, n_tomorrow) #production\n",
    "    # BC implies investment decision\n",
    "    x_tomorrow = y_tomorrow - c_tomorrow\n",
    "    k_tomorrow = (1 - params.delta)*k_t + x_tomorrow\n",
    "    penalty_tomorrow = penalty_factor*torch.maximum(torch.tensor([0.0]), -k_tomorrow )**2\n",
    "    sum_penalty = 0.5*penalty_t + 0.5*penalty_tomorrow\n",
    "    if torch.sum(sum_penalty) > 0.0:\n",
    "        print(f\"Sum penalty: {torch.sum(sum_penalty)}.\")\n",
    "    k_tomorrow = torch.maximum(tol, k_tomorrow)\n",
    "    ## euler error\n",
    "    # V1\n",
    "    if version_resid == 1:\n",
    "        ## euler error\n",
    "        R = params.β*((c_t/c_tomorrow)*(params.α*(y_tomorrow/k_t) + 1.0 - params.delta)) - 1.0\n",
    "    #V2. %diff between RHS and LHS side\n",
    "    elif version_resid == 2:\n",
    "        ## euler error\n",
    "        RHS = 1/c_t\n",
    "        LHS = params.β *((1/c_tomorrow)*(params.α*(y_tomorrow/k_t) + 1.0 - params.delta))\n",
    "        R = (RHS - LHS)/(0.5*RHS + 0.5*LHS) + sum_penalty\n",
    "    elif version_resid ==3:\n",
    "        ## euler error unit-free\n",
    "        R = 1/(params.β *((c_t/c_tomorrow)*(params.α*(y_tomorrow/k_t) + 1.0 - params.delta))) - 1.0\n",
    "    else:\n",
    "        raise(\"Error. version_resid unknwon.\")\n",
    "    return R #, R_FOC, R_FOC_tomorrow \n",
    "\n",
    "def simul_model(neural_net, params, N, tol=torch.tensor([1e-6])):\n",
    "    \"\"\"\n",
    "    Function to simulate the model. \n",
    "    N: length simulation\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        a_t = torch.zeros(N).unsqueeze(1)\n",
    "        #k_min_1 = torch.zeros(N).unsqueeze(1)\n",
    "        y_t = torch.zeros(N).unsqueeze(1)\n",
    "        k_t = torch.zeros(N).unsqueeze(1)\n",
    "        c_t = torch.zeros(N).unsqueeze(1)\n",
    "        n_t = torch.zeros(N).unsqueeze(1)\n",
    "        x_t = torch.zeros(N).unsqueeze(1)\n",
    "        # start at the non stochastic SS\n",
    "        a_t[0] = params.z_ss_tensor #non-stochastic SS\n",
    "        #k_min_1[0] = params.k_ss_tensor #non-stochastic SS\n",
    "        k_t[0] = params.k_ss_tensor #non-stochastic SS\n",
    "        y_t[0] = params.y_ss_tensor\n",
    "        c_t[0] = params.c_ss_tensor\n",
    "        n_t[0] = params.n_ss_tensor\n",
    "        x_t[0] = y_t[0] - c_t[0]\n",
    "        # Innovations\n",
    "        e_t = torch.normal(mean=0, std=params.σ_e, size=(N,)).unsqueeze(1)\n",
    "        # Generate path for a(t)\n",
    "        for i in range(1, N):\n",
    "            a_t[i] = torch.exp(params.rho*torch.log(a_t[i-1]) + e_t[i])\n",
    "        # Simulate economy\n",
    "        for i in range(1, N):\n",
    "            # Concat A and B\n",
    "            state_vec = torch.column_stack([k_t[i-1], a_t[i]])\n",
    "            c_t[i], n_t[i] = model_normalized(state_vec, neural_net, params)\n",
    "            y_t[i] = params.f(a_t[i], k_t[i-1], n_t[i]) #production\n",
    "            # BC implies investment decision\n",
    "            x_t[i] = y_t[i] - c_t[i]\n",
    "            # capital stock next period\n",
    "            # investment scaled by shock\n",
    "            k_t[i] = (1 - params.delta)*k_t[i-1] + x_t[i]\n",
    "    names = ['Z', 'K', 'Y', 'C', 'N']\n",
    "    series = torch.column_stack([a_t, k_t, y_t, c_t, n_t])\n",
    "    return series, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_MC_frozen(neural_net, params, tol=torch.tensor([1e-6]), use_linear = True, debug = False, distance_f = torch.abs):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectation\n",
    "    Use a pre-determined series of shocks to approximate the expectations\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        ## Period t\n",
    "        if use_linear == False:\n",
    "            c_t, n_t = model_normalized(params.k_min_1_and_a_t_accuracy, neural_net, params)\n",
    "            # squeeze\n",
    "            c_t = c_t.squeeze(1)\n",
    "            n_t = n_t.squeeze(1)\n",
    "        else:\n",
    "            c_t = params.c_linear(params.k_min1_accuracy, params.a_t_accuracy)\n",
    "            n_t = params.n_linear(params.k_min1_accuracy, params.a_t_accuracy)\n",
    "        y_t = params.f(params.a_t_accuracy, params.k_min1_accuracy, n_t) #production\n",
    "        if debug == True:\n",
    "            n_t = params.n_cst*torch.ones(c_t.shape)\n",
    "            y_t = params.f(params.a_t_accuracy, params.k_min1_accuracy, n_t) #production\n",
    "            c_t = params.c_delta_one(y_t)\n",
    "        # BC implies investment decision\n",
    "        x_t = y_t - c_t\n",
    "        # capital stock next period\n",
    "        # investment scaled by shock\n",
    "        k_t = (1 - params.delta)*params.k_min1_accuracy + x_t\n",
    "        k_t = torch.maximum(tol, k_t)\n",
    "\n",
    "        # repeat the vectors (MN*N, 1)\n",
    "        c_t_repeated = torch.kron(c_t, params.repeat_vector_accuracy)\n",
    "        n_t_repeated = torch.kron(n_t, params.repeat_vector_accuracy)\n",
    "        k_t_repeated = torch.kron(k_t, params.repeat_vector_accuracy)\n",
    "\n",
    "        ## Period t+1\n",
    "        # vectors are (MN*N, 1)\n",
    "        input_tomorrow = torch.column_stack([k_t_repeated, params.a_tomorrow_accuracy])\n",
    "        # consumption and hours worked tomorrow\n",
    "        if use_linear == False:\n",
    "            c_tomorrow, n_tomorrow = model_normalized(input_tomorrow, neural_net, params)\n",
    "            # squeeze\n",
    "            c_tomorrow = c_tomorrow.squeeze(1)\n",
    "            n_tomorrow = n_tomorrow.squeeze(1)\n",
    "        else:\n",
    "            c_tomorrow = params.c_linear(k_t_repeated, params.a_tomorrow_accuracy)\n",
    "            n_tomorrow = params.n_linear(k_t_repeated, params.a_tomorrow_accuracy)\n",
    "        y_tomorrow = params.f(params.a_tomorrow_accuracy, k_t_repeated, n_tomorrow) #production\n",
    "        if debug == True:\n",
    "            n_tomorrow = params.n_cst*torch.ones(c_tomorrow.shape)\n",
    "            y_tomorrow = params.f(params.a_tomorrow_accuracy, k_t_repeated, n_tomorrow) #production\n",
    "            c_tomorrow = params.c_delta_one(y_tomorrow)\n",
    "        # BC implies investment decision\n",
    "        x_tomorrow = y_tomorrow - c_tomorrow\n",
    "        k_tomorrow = (1 - params.delta)*k_t_repeated + x_tomorrow\n",
    "        k_tomorrow = torch.maximum(tol, k_tomorrow)\n",
    "\n",
    "        # Sparse matrix multiplication\n",
    "        vals = ((c_t_repeated/c_tomorrow)*(params.α*(y_tomorrow/k_t_repeated) + 1.0 - params.delta)).unsqueeze(1)\n",
    "        # Calculate expectation for each value of the state space (kt-1, at)\n",
    "        expect_t = params.β*torch.sparse.mm(params.W_accuracy, vals).squeeze(1)\n",
    "        euler_resid = distance_f(1.0 - (1.0/expect_t)) \n",
    "        # Euler error: \n",
    "    return euler_resid.numpy()\n",
    "\n",
    "def evaluate_accuracy_pytorch_Gaussian_frozen(neural_net, params, tol=torch.tensor([1e-6]), use_linear = True, debug = False, distance_f = torch.abs):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Gaussian quadrature for the expectation\n",
    "    Use a pre-determined series of shocks to approximate the expectations\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        ## Period t\n",
    "        if use_linear == False:\n",
    "            c_t, n_t = model_normalized(params.k_min_1_and_a_t_accuracy, neural_net, params)\n",
    "            # squeeze\n",
    "            c_t = c_t.squeeze(1)\n",
    "            n_t = n_t.squeeze(1)\n",
    "        else:\n",
    "            c_t = params.c_linear(params.k_min1_accuracy, params.a_t_accuracy)\n",
    "            n_t = params.n_linear(params.k_min1_accuracy, params.a_t_accuracy)\n",
    "        y_t = params.f(params.a_t_accuracy, params.k_min1_accuracy, n_t) #production\n",
    "        if debug == True:\n",
    "            n_t = params.n_cst*torch.ones(c_t.shape)\n",
    "            y_t = params.f(params.a_t_accuracy, params.k_min1_accuracy, n_t) #production\n",
    "            c_t = params.c_delta_one(y_t)\n",
    "        # BC implies investment decision\n",
    "        x_t = y_t - c_t\n",
    "        # capital stock next period\n",
    "        # investment scaled by shock\n",
    "        k_t = (1 - params.delta)*params.k_min1_accuracy + x_t\n",
    "        k_t = torch.maximum(tol, k_t)\n",
    "\n",
    "        # repeat the vectors of size (M,1) to be (MN,1) with (f1, f1, ..., f1, f2, f2, ..., fM, ... fM)\n",
    "        # N is the number of nodes for the gaussian quadrature\n",
    "        c_t_repeated = c_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        n_t_repeated = n_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        k_t_repeated = k_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        a_t_repeated = params.a_t_accuracy.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        #print(f\"shape c_t_repeated {c_t_repeated.shape}\")\n",
    "        #print(f\"shape a tomorrow: {n_t_repeated.shape}\")\n",
    "        #print(f\"shape k_t_repeated: {k_t_repeated.shape}\")\n",
    "        #print(f\"shape a_t_repeated: {a_t_repeated.shape}\")\n",
    "        # repeat (n1, n2, n3, n4, n1, n2, n3, n4, ... n1, n2, n3, n4).\n",
    "        # shape (MN, 1)\n",
    "        nodes_torch_repeated = params.nodes_torch.repeat(len(c_t)).unsqueeze(1)\n",
    "        # repeat (f1, f1, ..., f1, f2, f2, ..., fM, ... fM)\n",
    "        # shape (MN, 1)\n",
    "        # nodes_torch_repeated = params.nodes_torch.repeat_interleave(len(c_t)).unsqueeze(1)\n",
    "        # a(t) next period (MN, 1)\n",
    "        a_tomorrow = torch.exp(torch.log(a_t_repeated)*params.rho + nodes_torch_repeated)\n",
    "\n",
    "        ## Period t+1\n",
    "        # vectors are (MN*N, 1)\n",
    "        input_tomorrow = torch.column_stack([k_t_repeated, a_tomorrow])\n",
    "        #print(f\"shape input_tomorrow: {input_tomorrow.shape}\")\n",
    "        # consumption and hours worked tomorrow\n",
    "        if use_linear == False:\n",
    "            c_tomorrow, n_tomorrow = model_normalized(input_tomorrow.float(), neural_net, params)\n",
    "            # squeeze\n",
    "            #c_tomorrow = c_tomorrow.squeeze(1)\n",
    "            #n_tomorrow = n_tomorrow.squeeze(1)\n",
    "        else:\n",
    "            c_tomorrow = params.c_linear(k_t_repeated, a_tomorrow)\n",
    "            n_tomorrow = params.n_linear(k_t_repeated, a_tomorrow)\n",
    "        y_tomorrow = params.f(a_tomorrow, k_t_repeated, n_tomorrow) #production\n",
    "\n",
    "        if debug == True:\n",
    "            n_tomorrow = params.n_cst*torch.ones(c_tomorrow.shape)\n",
    "            y_tomorrow = params.f(a_tomorrow, k_t_repeated, n_tomorrow) #production\n",
    "            c_tomorrow = params.c_delta_one(y_tomorrow)\n",
    "        # BC implies investment decision\n",
    "        x_tomorrow = y_tomorrow - c_tomorrow\n",
    "        k_tomorrow = (1 - params.delta)*k_t_repeated + x_tomorrow\n",
    "        k_tomorrow = torch.maximum(tol, k_tomorrow)\n",
    "        #print(f\"shape c tomorrow: {c_tomorrow.shape}\")\n",
    "        #print(f\"shape y tomorrow: {y_tomorrow.shape}\")\n",
    "        #print(f\"shape a tomorrow: {a_tomorrow.shape}\")\n",
    "        #print(f\"shape k tomorrow: {k_tomorrow.shape}\")\n",
    "        #print(f\"shape n tomorrow: {n_tomorrow.shape}\")\n",
    "        #print(f\"shape nodes_torch_repeated: {nodes_torch_repeated.shape}\")\n",
    "        # Sparse matrix multiplication\n",
    "        vals = ((c_t_repeated/c_tomorrow)*(params.α*(y_tomorrow/k_t_repeated) + 1.0 - params.delta)).float()\n",
    "        # Calculate expectation for each value of the state space (kt-1, at)\n",
    "        expect_t = params.β*torch.sparse.mm(params.W_gaussian, vals).squeeze(1)\n",
    "        euler_resid = distance_f(1.0 - (1.0/expect_t))\n",
    "        #-----------------\n",
    "        # Euler error: \n",
    "    return euler_resid.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1962ece2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12280/578283457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_accuracy_pytorch_Gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_min1_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_t_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0musing\u001b[0m \u001b[0mGaussian\u001b[0m \u001b[0mquadrature\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexpectation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mHave\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy_pytorch_MC(neural_net, params, k_min1_accuracy, a_t_accuracy, nb_innovations, tol=torch.tensor([1e-6]), use_linear = True, debug = False, distance_f = torch.abs):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectation\n",
    "    Use new draws\n",
    "    \"\"\"\n",
    "    # Generate nb_draws shocks for each state\n",
    "    nb_states = len(k_min1_accuracy)\n",
    "    nb_innovations_tot = int(nb_states*nb_innovations)\n",
    "    e_r = generate_innovation_vec(nb_innovations_tot, params)\n",
    "    k_min_1_and_a_t_accuracy = torch.column_stack([k_min1_accuracy, a_t_accuracy])\n",
    "    \n",
    "    # Create integration matrix if necessary\n",
    "    if (nb_states != params.M_accuracy) | (nb_innovations != params.N_accuracy):\n",
    "        A = sparse.eye(nb_states)\n",
    "        B = sparse.csr_matrix(np.ones(nb_innovations)/nb_innovations)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        W_accuracy = sparse_mx_to_torch_sparse_tensor(sparse.kron(A, B))\n",
    "    else:\n",
    "        W_accuracy = params.W_accuracy\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        ## Period t\n",
    "        if use_linear == False:\n",
    "            c_t, n_t = model_normalized(k_min_1_and_a_t_accuracy, neural_net, params)\n",
    "            # squeeze\n",
    "            c_t = c_t.squeeze(1)\n",
    "            n_t = n_t.squeeze(1)\n",
    "        else:\n",
    "            c_t = params.c_linear(k_min1_accuracy, a_t_accuracy)\n",
    "            n_t = params.n_linear(k_min1_accuracy, a_t_accuracy)\n",
    "        y_t = params.f(a_t_accuracy, k_min1_accuracy, n_t) #production\n",
    "        if debug == True:\n",
    "            n_t = params.n_cst*torch.ones(c_t.shape)\n",
    "            y_t = params.f(a_t_accuracy, k_min1_accuracy, n_t) #production\n",
    "            c_t = params.c_delta_one(y_t)\n",
    "        # BC implies investment decision\n",
    "        x_t = y_t - c_t\n",
    "        # capital stock next period\n",
    "        # investment scaled by shock\n",
    "        k_t = (1 - params.delta)*k_min1_accuracy + x_t\n",
    "        k_t = torch.maximum(tol, k_t)\n",
    "\n",
    "        # repeat the vectors (MN*N, 1)\n",
    "        c_t_repeated = c_t.repeat_interleave(nb_innovations).unsqueeze(1)\n",
    "        n_t_repeated = n_t.repeat_interleave(nb_innovations).unsqueeze(1)\n",
    "        k_t_repeated = k_t.repeat_interleave(nb_innovations).unsqueeze(1)\n",
    "        a_t_repeated = a_t_accuracy.repeat_interleave(nb_innovations).unsqueeze(1)\n",
    "        \n",
    "        ## Period t+1\n",
    "        # vectors are (MN*N, 1)\n",
    "        a_tomorrow_accuracy = torch.exp(params.rho*torch.log(a_t_repeated) + e_r)\n",
    "        input_tomorrow = torch.column_stack([k_t_repeated, a_tomorrow_accuracy])\n",
    "        \n",
    "        # consumption and hours worked tomorrow\n",
    "        if use_linear == False:\n",
    "            c_tomorrow, n_tomorrow = model_normalized(input_tomorrow, neural_net, params)\n",
    "            # squeeze\n",
    "            c_tomorrow = c_tomorrow\n",
    "            n_tomorrow = n_tomorrow\n",
    "        else:\n",
    "            c_tomorrow = params.c_linear(k_t_repeated, a_tomorrow_accuracy)\n",
    "            n_tomorrow = params.n_linear(k_t_repeated, a_tomorrow_accuracy)\n",
    "        y_tomorrow = params.f(a_tomorrow_accuracy, k_t_repeated, n_tomorrow) #production\n",
    "        if debug == True:\n",
    "            n_tomorrow = params.n_cst*torch.ones(c_tomorrow.shape)\n",
    "            y_tomorrow = params.f(a_tomorrow_accuracy, k_t_repeated, n_tomorrow) #production\n",
    "            c_tomorrow = params.c_delta_one(y_tomorrow)\n",
    "        # BC implies investment decision\n",
    "        x_tomorrow = y_tomorrow - c_tomorrow\n",
    "        k_tomorrow = (1 - params.delta)*k_t_repeated + x_tomorrow\n",
    "        k_tomorrow = torch.maximum(tol, k_tomorrow)\n",
    "\n",
    "        # Sparse matrix multiplication\n",
    "        vals = (c_t_repeated/c_tomorrow)*(params.α*(y_tomorrow/k_t_repeated) + 1.0 - params.delta)\n",
    "\n",
    "        # Calculate expectation for each value of the state space (kt-1, at)\n",
    "        expect_t = params.β*torch.sparse.mm(W_accuracy, vals).squeeze(1)\n",
    "        euler_resid = distance_f(1.0 - (1.0/expect_t)) \n",
    "        \n",
    "        # Euler error: \n",
    "    return euler_resid.numpy()\n",
    "\n",
    "def evaluate_accuracy_pytorch_Gaussian(neural_net, params, k_min1_accuracy, a_t_accuracy, tol=torch.tensor([1e-6]), use_linear = True, debug = False, distance_f = torch.abs):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Gaussian quadrature for the expectation.\n",
    "    Have an input for kt-1 and at\n",
    "    \"\"\"\n",
    "    k_min_1_and_a_t_accuracy = torch.column_stack([k_min1_accuracy, a_t_accuracy])\n",
    "\n",
    "    if len(k_min1_accuracy) != params.M_accuracy:\n",
    "        # To calculate expectations\n",
    "        M_accuracy = len(k_min1_accuracy)\n",
    "        A_gaussian = sparse.eye(M_accuracy)\n",
    "        W_gaussian_local = sparse_mx_to_torch_sparse_tensor(sparse.kron(A_gaussian, params.B_gaussian))\n",
    "    else:\n",
    "        W_gaussian_local = params.W_gaussian\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        ## Period t\n",
    "        if use_linear == False:\n",
    "            c_t, n_t = model_normalized(k_min_1_and_a_t_accuracy, neural_net, params)\n",
    "            # squeeze\n",
    "            c_t = c_t.squeeze(1)\n",
    "            n_t = n_t.squeeze(1)\n",
    "        else:\n",
    "            c_t = params.c_linear(k_min1_accuracy, a_t_accuracy)\n",
    "            n_t = params.n_linear(k_min1_accuracy, a_t_accuracy)\n",
    "        y_t = params.f(a_t_accuracy, k_min1_accuracy, n_t) #production\n",
    "        if debug == True:\n",
    "            n_t = params.n_cst*torch.ones(c_t.shape)\n",
    "            y_t = params.f(a_t_accuracy, k_min1_accuracy, n_t) #production\n",
    "            c_t = params.c_delta_one(y_t)\n",
    "        # BC implies investment decision\n",
    "        x_t = y_t - c_t\n",
    "        # capital stock next period\n",
    "        # investment scaled by shock\n",
    "        k_t = (1 - params.delta)*k_min1_accuracy + x_t\n",
    "        k_t = torch.maximum(tol, k_t)\n",
    "\n",
    "        # repeat the vectors of size (M,1) to be (MN,1) with (f1, f1, ..., f1, f2, f2, ..., fM, ... fM)\n",
    "        # N is the number of nodes for the gaussian quadrature\n",
    "        c_t_repeated = c_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        n_t_repeated = n_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        k_t_repeated = k_t.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        a_t_repeated = a_t_accuracy.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        #print(f\"shape c_t_repeated {c_t_repeated.shape}\")\n",
    "        #print(f\"shape a tomorrow: {n_t_repeated.shape}\")\n",
    "        #print(f\"shape k_t_repeated: {k_t_repeated.shape}\")\n",
    "        #print(f\"shape a_t_repeated: {a_t_repeated.shape}\")\n",
    "        # repeat (n1, n2, n3, n4, n1, n2, n3, n4, ... n1, n2, n3, n4).\n",
    "        # shape (MN, 1)\n",
    "        nodes_torch_repeated = params.nodes_torch.repeat(len(c_t)).unsqueeze(1)\n",
    "        # repeat (f1, f1, ..., f1, f2, f2, ..., fM, ... fM)\n",
    "        # shape (MN, 1)\n",
    "        # nodes_torch_repeated = params.nodes_torch.repeat_interleave(len(c_t)).unsqueeze(1)\n",
    "        # a(t) next period (MN, 1)\n",
    "        a_tomorrow = torch.exp(torch.log(a_t_repeated)*params.rho + nodes_torch_repeated)\n",
    "\n",
    "        ## Period t+1\n",
    "        # vectors are (MN*N, 1)\n",
    "        input_tomorrow = torch.column_stack([k_t_repeated, a_tomorrow])\n",
    "        #print(f\"shape input_tomorrow: {input_tomorrow.shape}\")\n",
    "        # consumption and hours worked tomorrow\n",
    "        if use_linear == False:\n",
    "            c_tomorrow, n_tomorrow = model_normalized(input_tomorrow.float(), neural_net, params)\n",
    "            # squeeze\n",
    "            #c_tomorrow = c_tomorrow.squeeze(1)\n",
    "            #n_tomorrow = n_tomorrow.squeeze(1)\n",
    "        else:\n",
    "            c_tomorrow = params.c_linear(k_t_repeated, a_tomorrow)\n",
    "            n_tomorrow = params.n_linear(k_t_repeated, a_tomorrow)\n",
    "        y_tomorrow = params.f(a_tomorrow, k_t_repeated, n_tomorrow) #production\n",
    "\n",
    "        if debug == True:\n",
    "            n_tomorrow = params.n_cst*torch.ones(c_tomorrow.shape)\n",
    "            y_tomorrow = params.f(a_tomorrow, k_t_repeated, n_tomorrow) #production\n",
    "            c_tomorrow = params.c_delta_one(y_tomorrow)\n",
    "        # BC implies investment decision\n",
    "        x_tomorrow = y_tomorrow - c_tomorrow\n",
    "        k_tomorrow = (1 - params.delta)*k_t_repeated + x_tomorrow\n",
    "        k_tomorrow = torch.maximum(tol, k_tomorrow)\n",
    "        #print(f\"shape c tomorrow: {c_tomorrow.shape}\")\n",
    "        #print(f\"shape y tomorrow: {y_tomorrow.shape}\")\n",
    "        #print(f\"shape a tomorrow: {a_tomorrow.shape}\")\n",
    "        #print(f\"shape k tomorrow: {k_tomorrow.shape}\")\n",
    "        #print(f\"shape n tomorrow: {n_tomorrow.shape}\")\n",
    "        #print(f\"shape nodes_torch_repeated: {nodes_torch_repeated.shape}\")\n",
    "        # Sparse matrix multiplication\n",
    "        vals = ((c_t_repeated/c_tomorrow)*(params.α*(y_tomorrow/k_t_repeated) + 1.0 - params.delta)).float()\n",
    "        #print(f\"shape vals: {vals.shape}\")\n",
    "        # Calculate expectation for each value of the state space (kt-1, at)\n",
    "        expect_t = params.β*torch.sparse.mm(W_gaussian_local, vals).squeeze(1)\n",
    "        euler_resid = distance_f(1.0 - (1.0/expect_t))\n",
    "        # Evaluate errors\n",
    "        #-----------------\n",
    "        # Euler error: \n",
    "    return euler_resid.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to initialize the model\n",
    "# Fit the model when delta = 1\n",
    "def Ξ_torch_init(neural_net, params, init_type = 1): # objective function for training\n",
    "\n",
    "    # I.randomly drawing current states    \n",
    "    # A. values for beginning of period capital (kt-1)\n",
    "    if params.x_distribution == \"Uniform\":\n",
    "        k_min_1 = ((params.x_low - params.x_high) * torch.rand(params.T) + params.x_high).unsqueeze(1)\n",
    "    else:\n",
    "        k_min_1 = torch.normal(mean=params.mean_K_linear, std=params.stdev_K_linear, size=(params.T,)).unsqueeze(1)\n",
    "    # B. Values for a_t. Draw in ergodic set.\n",
    "    a_t = params.distrib_a_torch.sample((params.T,))\n",
    "    # Concat A and B\n",
    "    k_min_1_and_a_t = torch.column_stack([k_min_1, a_t])\n",
    "\n",
    "    c_t, n_t = model_normalized(k_min_1_and_a_t, neural_net, params)\n",
    "    \n",
    "    #output:\n",
    "    #V1. Predict the solution when delta=1.0\n",
    "    if init_type ==1:\n",
    "        y_t = params.f(a_t, k_min_1, params.n_cst)\n",
    "        c_delta_one = params.c_delta_one(y_t)\n",
    "        R = torch.mean((c_t - c_delta_one)**2) #+ torch.mean((n_t - params.n_cst)**2)\n",
    "    #V2. Predict the SS value\n",
    "    elif init_type ==2:\n",
    "        R = torch.mean((c_t - params.c_ss_tensor)**2) #+ torch.mean((n_t - params.n_ss_tensor)**2)\n",
    "    elif init_type ==3:\n",
    "    #V3. Predict the linearized model\n",
    "        ct_linear = params.c_linear(k_min_1, a_t)\n",
    "        nt_linear = params.n_linear(k_min_1, a_t)\n",
    "        R = torch.mean((c_t - ct_linear)**2) #+ torch.mean((n_t - nt_linear)**2)\n",
    "    elif init_type ==4:\n",
    "    #V4. sqrt(income)\n",
    "        y_t = params.f(a_t, k_min_1, params.n_cst)\n",
    "        #c_target = torch.log(y_t)\n",
    "        c_target = torch.sqrt(y_t)\n",
    "        R = torch.mean((c_t - c_target)**2) #+ torch.mean((n_t - 0.5*nt_linear)**2)\n",
    "    else:\n",
    "        raise(\"Unknown init_type\")\n",
    "    return R\n",
    "\n",
    "def create_optimizer(neural_net, params):\n",
    "    if params.optimizer == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(neural_net.parameters(), lr=params.lr) \n",
    "    elif params.optimizer == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(neural_net.parameters(), params.lr)\n",
    "    elif params.optimizer == \"SWA\":\n",
    "        base_opt = torch.optim.Adam(neural_net.parameters(), lr=params.lr) \n",
    "        optimizer = SWA(base_opt, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer {params.optimizer} unknown\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_effective_lr(optimizer, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Function to calculate the effective learning rate used in Adam update\n",
    "    \"\"\"\n",
    "    #effective_lr_list = []\n",
    "    effective_lr = 0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        for param in param_group['params']:\n",
    "            if param.grad is not None:\n",
    "                state = optimizer.state[param]\n",
    "                if 'step' in state and state['step'] > 0:\n",
    "                    m_t = state['exp_avg'] ## Exponential moving average of gradient values\n",
    "                    v_t = state['exp_avg_sq'] # Exponential moving average of squared gradient values\n",
    "                    # see: https://arxiv.org/pdf/1412.6980.pdf\n",
    "                    # page 2\n",
    "                    step_size = param_group['lr'] * np.sqrt(1 - beta2 ** state['step']) / (1 - beta1 ** state['step'])\n",
    "                    effective_lr = step_size / (torch.sqrt(v_t) + epsilon)\n",
    "                    # learning rate is applied to (1-beta1)*gradient\n",
    "                    effective_lr = np.mean((1 - beta1)*effective_lr.mean().item())\n",
    "                    #effective_lr = np.mean(effective_lr.mean().item())\n",
    "    return effective_lr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
