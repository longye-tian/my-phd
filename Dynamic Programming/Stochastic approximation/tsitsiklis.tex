\section{Theorem 1 and 3 of \cite{tsitsiklis1994asynchronous}}\label{sec:1}

In this section, we go through a simplified version of Theorem 1 and Theorem 3 of \cite{tsitsiklis1994asynchronous}. In particular, we omit the asynchronous algorithm part. First, we present the simplified setup in \cite{tsitsiklis1994asynchronous}\footnote{See \autoref{app:comparison} for direct comparison between the setup in \cite{tsitsiklis1994asynchronous} and the simplified setup}. Then, we use Robbins-Siegmund Theorem from \cite{robbins1971convergence}, i.e., \autoref{thm:robbins siegmund}, to explicitly prove Lemma 1 in \cite{tsitsiklis1994asynchronous} (see \autoref{lm:1}). Last, we present the proof of Theorem 1 (\autoref{thm:bounded}) and 3 (\autoref{thm: contraction}) of \cite{tsitsiklis1994asynchronous} in details.
\subsection{Simplified model setup and assumptions}\label{sub:simple setup}
Let $x(t)$ denote the state at discrete time $t\in\mathbb{N}$ with component $x_i(t)$. For each component, we have
$$
x_i(t+1) = (1-\alpha_i(t))x_i(t) + \alpha_i(t)(F_i(x(t)) + w_i(t)) 
$$
where
\begin{itemize}
    \item $\alpha_i(t) \in [0,1]$ is the stepsize parameter
    \item $w_i(t)$ is a noise term
\end{itemize}
All variables are defined on a probability space $(\Omega, \mathcal{F}, P)$ with an increasing sequence of $\sigma$-fields $\{\mathcal{F}(t)\}_{t=0}^{\infty}$ representing the algorithm's history. \\
\\
For any positive vector $v = (v_1, \ldots, v_n)$, we define the weighted maximum norm:
\begin{equation}\label{eq:weighted norm}
\|x\|_v = \max_i \frac{|x_i|}{v_i}, \quad x \in \mathbb{R}^n
\end{equation}
\noindent When $v = (1,\ldots,1)$, this is the standard maximum norm $\|\cdot\|_{\infty}$.\\
\noindent

\begin{assumption}[Statistical Properties]\label{ass:stat}
We assume
    \begin{enumerate}
\item[(a)] $x(0)$ is $\mathcal{F}(0)$-measurable;
\item[(b)] For every $i$ and $t$, $w_i(t)$ is $\mathcal{F}(t+1)$-measurable;
\item[(c)] For every $i$ and $t$, $\alpha_i(t)$ is $\mathcal{F}(t)$-measurable;
\item[(d)] For every $i$ and $t$, we have $\mathbb{E}[w_i(t) \mid \mathcal{F}(t)] = 0$;
\item[(e)] There exist constants $A$ and $B$ such that
$\mathbb{E}[w_i^2(t) \mid \mathcal{F}(t)] \leq A + B \max_j \max_{\tau \leq t} |x_j(\tau)|^2$, $\forall i, t$.
\end{enumerate}
\end{assumption}
\begin{assumption}[Stepsize conditions]\label{ass:stepsize}
We assume
\begin{enumerate}
\item[(a)] For every $i$, $\sum_{t=0}^{\infty} \alpha_i(t) = \infty$, w.p.1;
\item[(b)] There exists a constant $C$ such that for every $i$, $\sum_{t=0}^{\infty} \alpha_i^2(t) \leq C$, w.p.1.
\end{enumerate}
    
\end{assumption}
\begin{assumption} [Contraction]\label{ass:contraction}
There exists a vector $x^* \in \mathbb{R}^n$, a positive vector $v$, and a scalar $\beta \in [0,1)$, such that
\begin{equation}
\|F(x) - x^*\|_v \leq \beta \|x - x^*\|_v, \quad \forall x \in \mathbb{R}^n.
\end{equation}
    
\end{assumption}

\begin{assumption}[Boundedness]\label{ass:boundedness}
    There exists a positive vector $v$, a scalar $\beta \in [0,1)$, and a scalar $D$ such that
\begin{equation}
\|F(x)\|_v \leq \beta\|x\|_v + D, \quad \forall x \in \mathbb{R}^n.
\end{equation}
\end{assumption}

\begin{remark}\label{rm:3implies4}
    Notice that \autoref{ass:contraction} implies \autoref{ass:boundedness}:
        \begin{align*}
        \|F(x)\|_v &\le \|F(x)-x^*\|_v + \|x^*\|_v\tag{$\Delta$ ineq.}\\
        &\le \beta \|x-x^*\|_v+\|x^*\|_v\tag{\autoref{ass:contraction}}\\
        &\le \beta\|x\|_v + (1+\beta) \|x^*\|_v\tag{$\Delta$ ineq.}
    \end{align*}
    Let $D:= (1+\beta)\|x^*\|_v$
\end{remark}
\newpage
\subsection{Related Theorem}
In this section, we present the theorem related to the proof. Currently, we take this theorem as granted. Detailed proof is from \cite{robbins1971convergence} and here is a very nice blog post related to this theorem, see \href{https://nrhstat.org/post/robbins_siegmund/}{Why stochastic gradient descent works: 
The Robbins-Siegmund theorem on almost supermartingales}
\begin{theorem}[Robbins-Siegmund]\label{thm:robbins siegmund}
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $\{\mathcal{F}_n\}_{n=0}^{\infty}$ be a filtration. Let $\{V_n,\beta_n, \xi_n, \zeta_n\}_{n=0}^\infty$ be sequences of non-negative random variables adapted to $\{\mathcal{F}_n\}_{n=0}^{\infty}$ such that:
$$
\mathbb{E}[V_{n+1} \mid \mathcal{F}_n] \leq (1+\beta_n)V_n+\xi_n-\zeta_n\quad \text{a.s. for all } n \geq 0
$$
where
\begin{itemize}
    \item $\sum_{n=0}^\infty \beta_n<\infty$ almost surely
    \item $\sum_{n=0}^\infty \xi_n<\infty$ almost surely
\end{itemize}
Then:
\begin{itemize}
    \item $\lim_{n\to\infty}V_n = V_\infty$ exists and is finite almost surely
    \item $\sum_{n=0}^\infty \zeta_n<\infty$ almost surely
\end{itemize}
\end{theorem}

\subsection{Related Lemmas}
In the proof of \autoref{thm:bounded}, we need to show a noise process under certain conditions converges to zero. This motivates the following lemma. This lemma establishes conditions under which a stochastic process $W(t)$ converges to zero. The process follows the recursion:
$$
W(t+1) = (1-\alpha(t))W(t)  +\alpha(t) w(t)
$$
The proof is based on \autoref{thm:robbins siegmund}.
\begin{enumerate}
    \item We use the squared process $V(t) =W^2(t)$ and show that the squared process fits the condition of \autoref{thm:robbins siegmund}.
    \item Use \autoref{thm:robbins siegmund} to get convergence $V(t) \to V_\infty$
    \item Prove $V_\infty=0$ almost surely by contradiction, hence the original process converges to zero almost surely.
\end{enumerate}
\begin{lemma}\label{lm:1}
Let $\{\mathcal{F}(t)\}$ be an increasing sequence of $\sigma$-fields. For each $t$, let $\alpha(t)$, $w(t-1)$, and $B(t)$ be $\mathcal{F}(t)$-measurable scalar random variables. Let $C$ be a deterministic constant. Suppose that the following hold with probability 1:
\begin{enumerate}
\item[(a)] $\mathbb{E}[w(t) \mid \mathcal{F}(t)] = 0$;
\item[(b)] $\mathbb{E}[w^2(t) \mid \mathcal{F}(t)] \leq B(t)$;
\item[(c)] $\alpha(t) \in [0,1]$;
\item[(d)] $\sum_{t=0}^{\infty} \alpha(t) = \infty$;
\item[(e)] $\sum_{t=0}^{\infty} \alpha^2(t) \leq C$.
\end{enumerate}
Suppose that the sequence $\{B(t)\}$ is bounded with probability 1. Let $W(t)$ satisfy the recursion
\begin{equation}
W(t+1) = (1 - \alpha(t))W(t) + \alpha(t)w(t).
\end{equation}
Then $\lim_{t \to \infty} W(t) = 0$, with probability 1.
\end{lemma}

\begin{proof}
Let us first note that, without loss of generality, we can assume that $B(t) \leq K$ for some constant $K$ almost surely, since the sequence $\{B(t)\}$ is bounded with probability 1.\\
\\
\textbf{Step 1: Use the squared process}\\
\\
We analyze the evolution of the squared process $V(t) = W^2(t)$. From the recursion for $W(t)$, we have:
\begin{align*}
W(t+1) &= (1 - \alpha(t))W(t) + \alpha(t)w(t)
\end{align*}

\noindent Squaring both sides yields:
\begin{align*}
W^2(t+1) &= \left((1 - \alpha(t))W(t) + \alpha(t)w(t)\right)^2 \\
&= (1 - \alpha(t))^2W^2(t) + 2(1-\alpha(t))\alpha(t)W(t)w(t) + \alpha^2(t)w^2(t)
\end{align*}

\noindent Taking the conditional expectation with respect to $\mathcal{F}(t)$:
\begin{align*}
\mathbb{E}[W^2(t+1) \mid \mathcal{F}(t)] &= (1 - \alpha(t))^2W^2(t) + 2(1-\alpha(t))\alpha(t)W(t)\mathbb{E}[w(t) \mid \mathcal{F}(t)] + \alpha^2(t)\mathbb{E}[w^2(t) \mid \mathcal{F}(t)]
\end{align*}

\noindent Using the conditions $\mathbb{E}[w(t) \mid \mathcal{F}(t)] = 0$ and $\mathbb{E}[w^2(t) \mid \mathcal{F}(t)] \leq B(t)\le K$, we obtain:
\begin{align*}
\mathbb{E}[V(t+1) \mid \mathcal{F}(t)] &\leq (1 - \alpha(t))^2V(t) + \alpha^2(t)K \\
&= (1 - 2\alpha(t) + \alpha^2(t))V(t) + \alpha^2(t)K \\
&= V(t) - 2\alpha(t)V(t) + \alpha^2(t)V(t) + \alpha^2(t)K \\
&= V(t) - \alpha(t)V(t)(2 - \alpha(t)) + \alpha^2(t)K
\end{align*}

\noindent Since $\alpha(t) \in [0,1]$, we have $(2 - \alpha(t)) \geq 1$, which gives:
\begin{align*}
\mathbb{E}[V(t+1) \mid \mathcal{F}(t)] &\leq V(t) - \alpha(t)V(t) + \alpha^2(t)K \\
&= (1 - \alpha(t))V(t) + \alpha^2(t)K\\
&= V(t) + \alpha^2(t) K -\alpha(t) V(t)
\end{align*}
\textbf{Step 2: Use \autoref{thm:robbins siegmund}}\\
\\
\noindent Now, we let
\begin{itemize}
    \item $\xi_t = \alpha^2(t)K$, we have 
    $$
    \sum_{t=0}^\infty \xi_t  = \sum_{t=0}^\infty \alpha(t)^2K = K\sum_{t=0}^\infty \alpha^2(t) <\infty
    $$
    by our assumption. 
    \item $\zeta_t = \alpha(t)V(t)$ is nonnegative and adapted to the filtration. 
\end{itemize}
Hence, we use \autoref{thm:robbins siegmund}, we get
\begin{itemize}
    \item $\lim_{t\to\infty} V(t) = V_\infty$ exists and is finite almost surely
    \item $\sum_{t=0}^\infty \zeta_t = \sum_{t=0}^\infty \alpha(t) V(t)<\infty$ almost surely.
\end{itemize}
\textbf{Step 3: Prove $V_\infty =0$ almost surely by contradiction}\\
\\
Suppose that $P(V_\infty\ge 2\epsilon)>\delta$ for some $\epsilon, \delta>0$. Then we have on the set $\{\omega: V_\infty(\omega)\ge 2\epsilon\}$, by the definition of limit, for every $\omega\in \{\omega: V_\infty(\omega)\ge 2\epsilon\}$, there exists $T(\omega)\in\mathbb{N}$ such that for all $t\ge T(\omega)$, $V(t,\omega)\ge \epsilon$. Hence for all $\omega\in\{V_\infty\ge \epsilon\}$:
$$
\sum_{t=0}^\infty \zeta_t(\omega) = \sum_{t=0}^\infty \alpha(t)V(t,\omega)\ge \sum_{t=T(\omega)}^\infty \alpha(t) V(t,\omega)\ge \epsilon\sum_{t=T(\omega)}^\infty \alpha (t)
$$
By $\sum_{t=0}^\infty \alpha (t) = \infty$, we have $\sum_{t=T(\omega)}^\infty \alpha(t) = \infty$. Hence
$$
\sum_{t=0}^\infty \zeta_t(\omega) \ge \epsilon\sum_{t=T(\omega)}^\infty \alpha (t) = \infty
$$
This implies 
$$
\left\{\omega: \sum_{t=0}^\infty \zeta_t(\omega)=\infty\right\}\supseteq \{\omega: V_\infty(\omega)\ge 2\epsilon\}
$$
Hence
$$
P\left(\sum_{t=0}^\infty \zeta_t =  \infty\right) \ge P\left(V_\infty \ge 2\epsilon\right)>\delta
$$
This contradicts to $\sum_{t=0}^\infty \zeta_t<\infty$ almost surely.\\
\\
Hence, this contradiction gives $V_\infty =0$ almost surely.
\end{proof}
\newpage

\subsection{Main Theorems}
This section presents two main theorems from \cite{tsitsiklis1994asynchronous}. The first theorem \autoref{thm:bounded} is to show the stochastic process of interest as discussed in the setup is bounded under assumption 1,2, and 4.\\
\\
The second main theorem \autoref{thm: contraction} shows that this process converges to zero under assumption 1,2, and 3 which is based on the first theorem.
\subsubsection{\autoref{thm:bounded}}
In this section, we prove the first main theorem \autoref{thm:bounded}, which prove the process is bounded.  The strategy is proof by contradiction by assuming $x(t)$ is unbounded, in particular,
\begin{enumerate}
    \item Create a growing envelope $G(t)$ to track the growth of $x(t)$
    \item Use this tracking and growing envelope to normalize the noise and this normalized noise fits the condition of \autoref{lm:1}
    \item We use \autoref{lm:1} to show that the normalized noise converges to 0
    \item Setup the contradiction by selecting a time $t_0$ that the noise is very small for all $t\ge t_0$
    \item Derive the contradiction by showing the growing envelope is stablized after $t_0$ by induction
\end{enumerate}
\begin{theorem}\label{thm:bounded}
Let $(\Omega, \mathcal{F},P)$ be a probability space with filtration $\{\mathcal{F}_t\}_{t=0}^\infty$. Let $x(t)$ denote the state at discrete time $t\in\mathbb{N}$ with component $x_i(t)$. For each component, we have
$$
x_i(t+1) = (1-\alpha_i(t))x_i(t) + \alpha_i(t)(F_i(x(t)) + w_i(t)) 
$$
If \autoref{ass:stat},~\ref{ass:stepsize}, and~\ref{ass:boundedness} holds, then, the sequence $x(t)$ is bounded with probability 1.

\end{theorem}
\begin{proof}
\textbf{Step 0: Preliminary setup}\\
\\
First, we assume that we have already discarded a suitable null set, so we do not need to keep repeating the quanlification ``with probability 1''.\\
\\
We also assume that all components of the vector $v$ in \autoref{ass:boundedness}  are equal to $1$. (The case of a general positive weighting vector $v$ can be reduced to this special case by a suitable coordinate scaling.)\\
\\
In other words, we have there exists some $\beta\in [0,1)$ and some $D$ such that 
$$
\|F(x)\|_\infty \le \beta\|x\|_\infty  + D,\quad \forall x\in\mathbb{R}^n
$$
\\
\textbf{Step 1: Create a growing envelope $G(t)$ to monitor the growth of $x(t)$}\\
\\
We want to create a growing envelope $G(t)$ to monitor the growth of $x(t)$. Fix $G_0>0$ and $\gamma\in[0,1)$ such that
\begin{equation}\label{eq:Fandgamma}
    \|F(x)\|_\infty  \le \gamma \max\{\|x\|_\infty, G_0\},\quad\forall x\in\mathbb{R}^n
\end{equation}
(Any $\gamma\in[0,1)$ and $G_0>0$ satisfying $\beta G_0+D\le \gamma G_0$ will do.)\\
\\
Then, we fix $\epsilon>0$ such that $\gamma(1+\epsilon)=1$.
\begin{remark}
    Here $(1+\epsilon)$ is going to be the growth rate of the envelop. The choice of $\gamma(1+\epsilon)=1$ balances out the contraction and growth of the envelope which plays a key role in this proof.
\end{remark}

\noindent We want this envelope $G(t)$ to bound $x(t)$, we need to find the maximum of $x(t)$ up to some time $t$, hence, we define
\begin{equation}\label{eq:M(t)}
    M(t) = \max_{\tau\le t} \|x(\tau)\|_\infty 
\end{equation}
Now, we define a sequence $\{G(t)\}$, recursively. 
\begin{itemize}
    \item Let $G(0) = \max\{M(0), G_0\}$ be the initial bound
    \item Let 
    \begin{equation}\label{eq:updateruleofG}
           G(t+1) = \begin{cases}
        G(t) & \text{if $M(t+1)\le (1+\epsilon)G(t)$}\\
        G_0(1+\epsilon)^k &\text{if $M(t+1)>(1+\epsilon)G(t)$}
    \end{cases} 
    \end{equation}
    where $k$ is chosen so that 
    $$
    G_0(1+\epsilon)^{k-1}<M(t+1) \le G_0(1+\epsilon)^k
    $$
\end{itemize}
Under this construction, we create a growing envelope $G(t)$ such that
\begin{itemize}
    \item it stays constant unless the process exceeds $(1+\epsilon)$ times of the envelope
    \item Jumps to the next power of $(1+\epsilon)$ when exceeds this bound.
\end{itemize}
Under this construction, we also have 
\begin{equation}\label{eq:MtandGt}
    M(t)\le (1+\epsilon)G(t),\quad \forall t\ge 0
\end{equation}
and
\begin{equation}\label{eq:MtandGt2}
    M(t)\le G(t)\quad \text{if $G(t-1)<G(t)$}
\end{equation}

\noindent Also notice, that under this contruction, $\{G(t)\}_{t=0}^\infty$ is a strictly positive increasing sequence, i.e.,
\begin{equation}
   0< G_0\le G(0) \le G(1) \le \cdots
\end{equation}\label{eq:G(t)increasing}
\textbf{Step 2: Use this envelope to normalize the noise $w(t)$}\\
\\
Moreover, we have $M(t),G(t)$ are all $\mathcal{F}(t)$-measurable. Next, we define
$$
\tilde w_i(t) = \frac{w_i(t)}{G(t)},\quad \forall t\ge 0
$$
which is $\mathcal{F}(t+1)$-measurable. Under \autoref{ass:stat}, we have
$$
\mathbb{E}(\tilde w_i(t)|\mathcal{F}(t)) = \frac{\mathbb{E}(w_i(t)|\mathcal{F}(t))}{G(t)} = 0
$$
and
\begin{align*}
    \mathbb{E}(\tilde w_i^2(t)|\mathcal{F}(t)) &= \frac{\mathbb{E}(w_i^2(t)|\mathcal{F}(t))}{G^2(t)}\\
    &\le \frac{A + B \max_j\max_{\tau\le t}|x_j(\tau)|^2}{G^2(t)}\tag{\autoref{ass:stat}}\\
    &= \frac{A + B M(t)^2}{G^2(t)}\tag{\autoref{eq:M(t)}}\\
    &\le \frac{A+B(1+\epsilon)^2G^2(t)}{G^2(t)}\tag{\autoref{eq:MtandGt}}\\
    &= \frac{A}{G^2(t)} + B(1+\epsilon)^2 \\
    &\le \frac{A}{G_0^2} + B(1+\epsilon)^2 \tag{\autoref{eq:G(t)increasing}}\\
    &=:K\quad \forall t\ge 0
\end{align*}
where $K$ is some deterministic constant.\\
\\
\textbf{Step 3: create a recursive structure to use \autoref{lm:1}}\\
\\
For any $i$ and $t_0\ge 0$, we define $\tilde W_i(t_0;t_0) = 0$ and
\begin{equation}\label{eq:tildeW}
    \tilde W_i(t+1; t_0) = (1-\alpha_i(t))\tilde W_i(t;t_0) + \alpha_i(t)\tilde w_i(t),\quad \forall t\ge 0
\end{equation}
\noindent Under this definition, we iterate to get the expression for $\tilde W_i(t;0)$ as
$$
\tilde W_i(t;0) = \left[\prod_{\tau = t_0}^{t-1} (1-\alpha_i(\tau))\right]\tilde W_i(t_0;0) + \tilde W_i(t;t_0)
$$
for every $t\ge t_0$. This implies
$$
|\tilde W_i(t;t_0)|\le |\tilde W_i(t;0)| + |\tilde W_i(t_0;0)|
$$
By \autoref{lm:1}, we have 
$$
\lim_{t\to\infty} \tilde W_i(t;0) = 0
$$
Hence, we have for every $\delta >0$, there exists some $T$ such that $|\tilde W_i(t;t_0)|\le \delta$, for every $t$ and $t_0$ satisfying $T\le t_0\le t$.\\
\\
\textbf{Step 4: Setup the contradiction}\\
\\
Now we prove that $x(t)$ is bounded by contradiction. Suppose that $x(t)$ is unbounded. The by \autoref{eq:M(t)} and \autoref{eq:MtandGt}, we have the tracking envelope $G(t)$ goes to infinity.\\
\\
Then by the construction of $G(t)$ and \autoref{eq:MtandGt2}, the inequality $M(t)\le G(t)$ holds for infinitely many different values of $t$.\\
\\
Moreover, since $G(t)$ goes to infinity and by \autoref{eq:MtandGt2}, there exists some $t_0$ such that for all $t\ge t_0$ the process is bounded by the growing envelope and the noise is very small, i.e.,
$M(t_0)\le G(t_0)$ and
\begin{equation}\label{eq:tildeWgoesto0}
    |\tilde W_i(t;t_0)|\le \epsilon,\quad \forall t\ge t_0,\,\,\forall i
\end{equation}
\textbf{Step 5: Derive the contradiction by showing $G(t)$ stablizes after $t_0$ via induction}\\
\\
Now we show by induction that for every $t\ge t_0$, we have $G(t)=G(t_0)$ and for every $i$, we have
$$
-G(t_0)(1+\epsilon) \le -G(t_0) + \tilde W_i(t;t_0)G(t_0)\le x_i(t)\le G(t_0) + \tilde W_i(t;t_0)G(t_0)\le G(t_0)(1+\epsilon)
$$
\\
\textbf{Base Case $t=t_0$}\\
\\
We start with the case for $t=t_0$, we have
$$
|x_i(t_0)|\le M(t_0)\le G(t_0)
$$
and
$$
\tilde W_i(t_0;t_0) = 0
$$
Hence, we have
$$
-G(t_0)(1+\epsilon)\le -G(t_0)\le x_i(t) \le G(t_0)\le G(t_0)(1+\epsilon)
$$
as discussed before, i.e., $x(t)$ is inside the tracking and growing envelope $G(t)$ at $t=t_0$.\\
\\
\textbf{Induction hypothesis:}\\
\\
Suppose the result is true for some time $t>t_0$. \\
\\
\textbf{Induction Case at $t+1$}\\
\\
We use this induction hypothesis and \autoref{eq:Fandgamma}, we get
\begin{align*}
    x_i(t+1) &= (1-\alpha_i(t))x_i(t) + \alpha_i(t)F_i(x(t)) + \alpha_i(t)w_i(t)\\
    &\le (1-\alpha_i(t))(G(t_0) + \tilde W_i(t;t_0)G(t_0)) + \alpha_i(t)F_i(x(t)) + \alpha_i(t)w_i(t)\tag{Induction}\\
    &\le (1-\alpha_i(t))(G(t_0) + \tilde W_i(t;t_0)G(t_0)) + \alpha_i(t)\gamma\max\{\|x\|_\infty,G_0\} + \alpha_i(t)w_i(t)\tag{\autoref{eq:Fandgamma}}\\
    &\le (1-\alpha_i(t))(G(t_0) + \tilde W_i(t;t_0)G(t_0)) + \alpha_i(t)\gamma G(t_0)(1+\epsilon)+ \alpha_i(t)w_i(t)\tag{Induction}\\
    &\le (1-\alpha_i(t))(G(t_0) + \tilde W_i(t;t_0)G(t_0)) + \alpha_i(t)\gamma G(t_0)(1+\epsilon)+ \alpha_i(t)\tilde w_i(t)G(t_0)\tag{Induction}\\
    & = G(t_0) + ((1-\alpha_i(t))\tilde W_i(t;t_0)+\alpha_i(t)\tilde w_i(t))G(t_0)\tag{$\gamma(1+\epsilon)=1$}\\
    & = G(t_0)+ \tilde W_i(t+1;t_0)G(t_0) \tag{\autoref{eq:tildeW}}
\end{align*}
Symmetrically, we can get the other direction.\\
\\
Then, by \autoref{eq:tildeWgoesto0}, we get
$$
|x_i(t+1)|\le G(t_0)(1+\epsilon)
$$
Hence, we have
$$
\|x(t+1)\|_\infty \le G(t_0)(1+\epsilon)
$$
By \autoref{eq:M(t)}, we have $M(t+1) = \max\{M(t), \|x(t+1)\|_\infty\}$. And by the induction hypothesis, we have $G(t) = G(t_0)$ for some $t\ge t_0$. Hence, by \autoref{eq:MtandGt2}, we have
$$
M(t) \le (1+\epsilon)G(t) = G(t_0)(1+\epsilon)
$$
This implies
\begin{align*}
    M(t+1) &= \max\{M(t),\|x(t+1)\|_\infty\}\\
    &\le \max\{G(t_0)(1+\epsilon), G(t_0)(1+\epsilon)\}\\
    &= G(t_0)(1+\epsilon)\\
    & = G(t)(1+\epsilon)
\end{align*}
Then, by the update rule, i.e., \autoref{eq:updateruleofG}, we have $G(t+1) = G(t) = G(t_0)$.\\
\\
This contradicts to $G(t)$ goes to infinity, hence $x(t)$ is bounded.
\end{proof}
\newpage
\subsubsection{\autoref{thm: contraction}}
This is the second main theorem we are interested in, that this process converges to $x^*$ under the contraction assumption, i.e., \autoref{ass:contraction}.\\
\\
The proof of this theorem uses a nested induction approach. The structure of the proof follows
\begin{enumerate}
    \item Show that $x(t)$ is bounded using \autoref{thm:bounded}
    \item Create a sequence of decreasing bounds $D_0, D_1, D_2,\cdots$ that converges to zero
    \item Prove using induction that for each $k$, the proess eventually stays within the bounds given by $D_k$, this is the outer induction.
    \item To prove the induction step in the outer induction, we use an inner induction to show that the process eventually moves to $D_{k+1}$.
\end{enumerate}
\begin{theorem}\label{thm: contraction}
Let $(\Omega, \mathcal{F},P)$ be a probability space with filtration $\{\mathcal{F}_t\}_{t=0}^\infty$. Let $x(t)$ denote the state at discrete time $t\in\mathbb{N}$ with component $x_i(t)$. For each component, we have
$$
x_i(t+1) = (1-\alpha_i(t))x_i(t) + \alpha_i(t)(F_i(x(t)) + w_i(t)) 
$$
If \autoref{ass:stat},~\ref{ass:stepsize}, and~\ref{ass:contraction} holds,then, the sequence $x(t)$ converges to $x^*$ with probability 1.
\end{theorem}
\begin{proof}
\textbf{Step 1: Show that $x(t)$ is bounded using \autoref{thm:bounded}}\\
\\
   Notice that \autoref{ass:contraction} implies \autoref{ass:boundedness}.
        \begin{align*}
        \|F(x)\|_v &\le \|F(x)-x^*\|_v + \|x^*\|_v\tag{$\Delta$ ineq.}\\
        &\le \beta \|x-x^*\|_v+\|x^*\|_v\tag{\autoref{ass:contraction}}\\
        &\le \beta\|x\|_v + (1+\beta) \|x^*\|_v\tag{$\Delta$ ineq.}
    \end{align*}
    Let $D:= (1+\beta)\|x^*\|_v$. Then we can apply \autoref{thm:bounded}, we get $x(t)$ is bounded with probabilty 1. (W.l.o.g., we let $x^*=0$).\\
    \\
    \textbf{Step 2: Create a sequence of decreasing bounds $D_0, D_1,\cdots$ that converges to $0$}\\
    \\
    Hence there exists some (generally random) $D_0$ such that 
    $$
    \|x(t)\|_\infty \le D_0,\quad \forall t\ge 0
    $$
    Fix some $\epsilon>0$ such that $\beta(1+2\epsilon)<1$, we define
    \begin{equation}\label{eq:Dkrecursive}
        D_{k+1} = \beta(1+2\epsilon)D_k,\quad k\ge 0
    \end{equation}
    Clearly, $D_k$ converges to $0$.\\
    \\
    \textbf{Step 3: Prove using induction that for each $k$, the proess eventually stays within the bounds given by $D_k$}\\
    \\
    Now we prove by induction to show that for each $k$, there exists a time $t_k$ such that $\|x(t)\|_\infty\le D_k$ for all $t\ge t_k$.\\
    \\
    \textbf{Base case $k=0$}:\\
    This case has already been shown above.\\
    \\
    \textbf{Induction hypothesis}:\\
    Assume there exists $t_k$ such that $\|x(t)\|_\infty \le D_k$ for all $t\ge t_k$.\\
    \\
    \textbf{Induction step at $k+1$}\\
    We need to show there exists $t_{k+1}\ge t_k$ such that 
    $$
    \|x(t)\|_\infty \le D_{k+1},\quad \forall t\ge t_{k+1}
    $$
    We define a sequence $W_i(t)$ to track the accumulated noise:
    \begin{itemize}
        \item $W_i(0)$ = 0
        \item $W_i(t+1) = (1-\alpha_i(t))W_i(t) + \alpha_i(t)w_i(t)$
    \end{itemize}
    As previously shown in the proof of \autoref{thm:bounded}, for every $\delta>0$, there exists a time $T$ such that
    \begin{equation}\label{eq:th3Wt}
         |W_i(t;t_0)|\le \delta,\quad \forall T\le t_0\le t
    \end{equation}
    Choose $\tau_k\ge t_k$ such that
    \begin{itemize}
        \item $|W_i(t;\tau_k)|\le \beta\epsilon D_k$ for all $t\ge \tau_k$ by \autoref{eq:th3Wt}\\
        \item $\|x(t)\|_\infty \le D_k$ for all $t\ge \tau_k\ge t_k$ by the induction hypothesis
    \end{itemize}
    Then we can define a sequence that provides an upper bound, 
    \begin{itemize}
        \item Let $Y_i(\tau_k) = D_k$
        \item Let 
        \begin{equation}\label{eq:Yrecusive}
            Y_i(t+1) = (1-\alpha_i(t))Y_i(t)+\alpha_i(t)\beta D_k
        \end{equation}
        for all $t\ge \tau_k$
    \end{itemize}
    \textbf{Step 4: Inner induction}\\
    \\
    Now we prove that 
    \begin{equation}\label{eq:lemma8induction}
     -Y_i(t) + W_i(t;\tau_k)\le x_i(t)\le Y_i(t) + W_i(t;\tau_k),\quad \forall t\ge \tau_k   
    \end{equation}
    by induction.\\
    \\
    \textbf{Inner induction: Base case $t= \tau_k$}\\
    This is satisfied by $\|x(t)\|_\infty \le D_k$ as $Y_i(\tau_k)=D_k$ and $W_i(\tau_k;\tau_k)=0$.\\
    \\
    \textbf{Inner induction hypothesis}:\\
    Suppose that \autoref{eq:lemma8induction} holds for some $t>\tau_k$. \\
    \\
    \textbf{Inner induction step: }\\
    Then, we have
    \begin{align*}
         x_i(t+1) &= (1-\alpha_i(t))x_i(t) + \alpha_i(t)F_i(x(t)) + \alpha_i(t)w_i(t)\\
         &\le (1-\alpha_i(t))(Y_i(t) + W_i(t;\tau_k)) + \alpha_i(t)F_i(x(t)) + \alpha_i(t)w_i(t)\tag{Inner Induction}\\
         &\le (1-\alpha_i(t))(Y_i(t) + W_i(t;\tau_k)) + \alpha_i(t)\beta\|x(t)\|_\infty + \alpha_i(t)w_i(t)\tag{\autoref{ass:contraction}}\\
         &\le (1-\alpha_i(t))(Y_i(t) + W_i(t;\tau_k)) + \alpha_i(t)\beta D_k + \alpha_i(t)w_i(t)\tag{$\|x(t)\|_\infty \le D_k$}\\
         & =[(1-\alpha_i(t))Y_i(t) + \alpha_i(t)\beta D_k] + [(1-\alpha_i(t)) W_i(t;\tau_k) + \alpha_i(t)w_i(t)]\\
         &= Y_i(t+1) + W_i(t+1;\tau_k)
    \end{align*}
    Symmetrically, we can show the other direction.\\
    \\
    Hence, we have
    $$
    -Y_i(t+1) + W_i(t+1)\le x_i(t+1)\le Y_i(t+1) + W_i(t+1;\tau_k),\quad\forall t\ge \tau_k
    $$
    Hence, by induction we prove \autoref{eq:lemma8induction}. \\
    \\
    \textbf{Finally, we use the result from the inner induction: }\\
    \\
    Moreover since $Y_i(t)$ are positive, we can get
    \begin{equation}\label{eq:xiboundedlast}
        |x_i(t)| \le Y_i(t) + |W_i(t;\tau_k)|
    \end{equation}
    By \autoref{eq:Yrecusive}, we have
    \begin{align*}
        Y_i(t+1) -\beta D_k &= (1-\alpha_i(t))Y_i(t)+\alpha_i(t)\beta D_k -\beta D_k\\
        Z_i(t+1) &= (1-\alpha_i(t))Y_i(t)-(1-\alpha_i(t))\beta D_k\tag{$Z_i(t):= Y_i(t)-\beta D_k$}\\
        Z_i(t+1) &= (1-\alpha_i(t)) Z_i(t)
    \end{align*}
    Hence, by \autoref{ass:stepsize}, we get $Y_i(t)\to \beta D_k$ as $t\to \infty$.\\
    \\
    Since $\limsup$ always exists for bounded sequence, we have
    \begin{equation}
        \limsup_{t\to\infty} |x_i(t)| \le \limsup_{t\to\infty} (Y_i(t) + |W_i(t;\tau_k)|)
    \end{equation}
    Use the property that the limsup of a sum is at most the sum of the limsup, we get
    $$
    \limsup_{t\to\infty} (Y_i(t) + |W_i(t;\tau_k)|)\le \limsup_{t\to\infty} Y_i(t) + \limsup_{t\to\infty} |W_i(t;\tau_k)|
    $$
    As $Y_i(t)\to \beta D_k$, we have $\limsup_{t\to \infty} Y_i(t) = \beta D_k$. Moreover, since we have $|W_i(t;\tau_k)|\le \beta\epsilon D_k$ for all $t\ge \tau_k$ by \autoref{eq:th3Wt}. We have, in all,
    $$
    \limsup_{t\to\infty} |x_i(t)| \le \beta D_k  + \beta \epsilon D_k = \beta(1+\epsilon) D_k<\beta(1+2\epsilon) D_k =D_{k+1}
    $$
    by \autoref{eq:Dkrecursive}. Hence, this implies there exist $t$ large enough, $|x_i(t)|<D_{k+1}$. This proves the claim.\\
    \\
    Hence, we have $\|x(t)\|_\infty\le D_k$ for all $t\ge t_k$ and $D_k$ converges to $0$. This implies $x(t)$ converges to $0$.
\end{proof}
